# Machine-Learning-Regularization-Techniques

This repository contains Python code showcasing different approaches to enhance the performance of a digit classification model using the MNIST dataset. We explore various regularization techniques, dropout, early stopping, and data augmentation to improve the accuracy of the model.

## Methods and Techniques
L1 Regularization (Lasso Regression) : Logistic regression with L1 regularization (Lasso) to improve model accuracy.

L2 Regularization (Ridge Regression) : Logistic regression with L2 regularization (Ridge) for comparison and evaluation.

Elastic Net Regularization : Logistic regression with Elastic Net regularization, blending L1 and L2 penalties.

Dropout : Neural network with dropout layers for enhanced model robustness and accuracy.

Early Stopping : Utilization of early stopping technique to prevent overfitting and optimize accuracy.

Data Augmentation : Implementation of data augmentation using ImageDataGenerator for improved model generalization and accuracy.

## Usage

The code is organized into separate Python files for each method, making it easy to understand and run individually.
Execute each file to observe the results and accuracy achieved using the respective technique.

## Contributor

Arash Bakhtiary 

## License

This project is licensed under the MIT License.

Feel free to contribute, open issues, and provide suggestions to improve this project!
